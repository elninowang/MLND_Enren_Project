{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# P5: 从安然公司邮件中发现欺诈证据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elnin\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务0: 浏览数据级，看看数据集大致是什么样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  146  poi_count:  18 features count: 20\n",
      "all features coount: ['salary', 'to_messages', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'email_address', 'restricted_stock_deferred', 'total_stock_value', 'shared_receipt_with_poi', 'long_term_incentive', 'exercised_stock_options', 'from_messages', 'other', 'from_poi_to_this_person', 'from_this_person_to_poi', 'poi', 'deferred_income', 'expenses', 'restricted_stock', 'director_fees']\n"
     ]
    }
   ],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "count, poi_count = 0,0\n",
    "all_features_list = set()\n",
    "for k,v in data_dict.items():\n",
    "    if v['poi'] == True: poi_count += 1\n",
    "    count += 1\n",
    "    for k1 in v.keys():\n",
    "        if not k1 in all_features_list:\n",
    "            all_features_list.add(k1)\n",
    "all_features_list = list(all_features_list)            \n",
    "print 'count: ',count,\" poi_count: \",poi_count, \"features count:\", len(all_features_list)-1\n",
    "print \"all features coount:\", all_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集中共有人数 ** 146 ** 人，其中涉嫌欺诈的有 **18** 人, 特征个数是 **20** 个\n",
    "\n",
    "基于数据集具有以下特点，大致思路如下：\n",
    "\n",
    "- 这个数据集很不平衡（imbalance）, 也就说明accuracy并不是很好的评估指标，选择precision和recall更好一些。\n",
    "- 在交叉验证的时候，因为数据的不平衡性，我们会选用Stratified Shuffle Split的方式将数据分为验证集和测试集。\n",
    "- 数据样本比较少，因此我们可以使用GridSearchCV来进行参数调整，如果较大的数据则会花费较长的时间，可以考虑使用RandomizedSearchCV。\n",
    "\n",
    "看看一个典型用户有哪些特征，其值是多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary  :  365788\n",
      "to_messages  :  807\n",
      "deferral_payments  :  NaN\n",
      "total_payments  :  1061827\n",
      "exercised_stock_options  :  NaN\n",
      "bonus  :  600000\n",
      "restricted_stock  :  585062\n",
      "shared_receipt_with_poi  :  702\n",
      "restricted_stock_deferred  :  NaN\n",
      "total_stock_value  :  585062\n",
      "expenses  :  94299\n",
      "loan_advances  :  NaN\n",
      "from_messages  :  29\n",
      "other  :  1740\n",
      "from_this_person_to_poi  :  1\n",
      "poi  :  False\n",
      "director_fees  :  NaN\n",
      "deferred_income  :  NaN\n",
      "long_term_incentive  :  NaN\n",
      "email_address  :  mark.metts@enron.com\n",
      "from_poi_to_this_person  :  38\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict['METTS MARK'].items():\n",
    "    print k, \" : \", v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1: 选择你需要的特征\n",
    "\n",
    "选择所有的财务特征, 其中  email_address 是没有任何数据信息，所以可以考虑去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi']\n",
    "# finance feature\n",
    "features_list += ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', \n",
    "                  'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', \n",
    "                  'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', \n",
    "                  'director_fees']\n",
    "# email feature\n",
    "features_list +=  ['to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "print len(features_list)\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "y, X = targetFeatureSplit(featureFormat(data_dict, features_list))\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试试用决策树大致筛选一下对数据的敏感性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.feature_importances_ :  [ 0.          0.          0.16475213  0.          0.17958795  0.          0.0543682\n",
      "  0.          0.06221847  0.05858439  0.08072355  0.10873641  0.12036185\n",
      "  0.          0.          0.08721566  0.          0.08345138  0.        ]\n",
      "importance  0  -  bonus  -  0.179587954314\n",
      "importance  1  -  total_payments  -  0.164752133256\n",
      "importance  2  -  restricted_stock  -  0.120361849464\n",
      "importance  3  -  long_term_incentive  -  0.108736407949\n",
      "importance  4  -  from_poi_to_this_person  -  0.0872156605424\n",
      "importance  5  -  from_this_person_to_poi  -  0.0834513822454\n",
      "importance  6  -  other  -  0.0807235517364\n",
      "importance  7  -  expenses  -  0.0622184676102\n",
      "importance  8  -  exercised_stock_options  -  0.0585843889079\n",
      "importance  9  -  deferred_income  -  0.0543682039745\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=12)\n",
    "clf = clf.fit(X, y)\n",
    "print \"clf.feature_importances_ : \", clf.feature_importances_\n",
    "idx_feature_importances = np.argsort(clf.feature_importances_)[::-1]\n",
    "for i in range(10):\n",
    "    idx = idx_feature_importances[i]\n",
    "    print \"importance \", i, \" - \", features_list[idx+1], \" - \", clf.feature_importances_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可以看出，和poi关系大的，依次是\n",
    "\n",
    "- bonus: 奖金\n",
    "- total_payments: 总收入\n",
    "- restricted_stock: 受限股票\n",
    "- expenses: 花费\n",
    "- long_term_incentive: 长期激励\n",
    "- from_poi_to_this_person: 从poi邮箱发到此邮箱的邮件数量\n",
    "- exercised_stock_options: 行权数量\n",
    "- from_this_person_to_poi: 从此邮箱发到poi邮箱的邮件数量\n",
    "\n",
    "可以看出比较奇特的是，工资 salary 居然对结果影响不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'bonus', 'total_payments', 'restricted_stock', 'long_term_incentive', 'from_poi_to_this_person', 'from_this_person_to_poi', 'other', 'expenses', 'exercised_stock_options', 'deferred_income']\n"
     ]
    }
   ],
   "source": [
    "new_features_list = ['poi']\n",
    "for i in range(10):\n",
    "    idx = idx_feature_importances[i]\n",
    "    new_features_list.append(features_list[idx+1])\n",
    "print new_features_list\n",
    "features_list = new_features_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2: 删除异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于上面发现的奇怪现象，这里首先检查salary的异常值。\n",
    "\n",
    "这里先检查 salary 和 bonus里面 有没有NaN的。如果看看，是否存在POI人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAGER F SCOTT  -  158403   NaN\n",
      "HIRKO JOSEPH  -  NaN   NaN\n",
      "\n",
      "nan_salary_count:  51  nan_bonus_count:  64\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "\n",
    "nan_salary_count = 0\n",
    "nan_bonus_count = 0\n",
    "poi_count_when_nan_salary_or_nan_bonu = 0\n",
    "for k,v in data_dict.items():\n",
    "    if v[\"salary\"] == 'NaN': nan_salary_count +=1\n",
    "    if v[\"bonus\"] == 'NaN': nan_bonus_count += 1\n",
    "    if v[\"salary\"] == 'NaN'or v[\"bonus\"] == 'NaN':\n",
    "        if v[\"poi\"] == True:\n",
    "            print k,\" - \", v[\"salary\"], \" \", v[\"bonus\"]\n",
    "print\n",
    "print \"nan_salary_count: \", nan_salary_count, \" nan_bonus_count: \", nan_bonus_count      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对所有人的名字做一下扫描，看看有没有异常的名字，名字中1到2个空格，我们打印出名字中空格书不是1或者2的人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WALLS JR ROBERT H\n",
      "BOWEN JR RAYMOND M\n",
      "OVERDYKE JR JERE C\n",
      "PEREIRA PAULO V. FERRAZ\n",
      "BLAKE JR. NORMAN P\n",
      "THE TRAVEL AGENCY IN THE PARK\n",
      "TOTAL\n",
      "WHITE JR THOMAS E\n",
      "WINOKUR JR. HERBERT S\n",
      "DERRICK JR. JAMES V\n",
      "DONAHUE JR JEFFREY M\n",
      "GLISAN JR BEN F\n"
     ]
    }
   ],
   "source": [
    "for name,_ in data_dict.items():\n",
    "    mini_names = name.split(' ')\n",
    "    if len(mini_names) != 2 and len(mini_names) != 3:\n",
    "        print name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在其结果中看看，可以看出两个异常，一个是 `THE TRAVEL AGENCY IN THE PARK` 一个是 `TOTAL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查每个人的各个属性，看看有没有绝大部分属性都是 NaN 的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E  -  20\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict.items():\n",
    "    nanCount = 0\n",
    "    for featurn,value in v.items():\n",
    "        if value == 'NaN': \n",
    "            nanCount += 1\n",
    "    if nanCount > 18:\n",
    "        print k, \" - \", nanCount\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现 LOCKHART EUGENE E 的20个属性都是NaN，这个数据完全没有一样，也应该作为异常值删掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现存在了不少 工资 或者 奖金 NaN, 但是这些里面存在POI人，不能说明是异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bonus  97343619.0\n",
      "the max bonus person is  TOTAL\n"
     ]
    }
   ],
   "source": [
    "bonus_idx = 4\n",
    "\n",
    "max_bonus = np.amax(X[:,bonus_idx])\n",
    "print \"max_bonus \", max_bonus\n",
    "for k,v in data_dict.items():\n",
    "    if v[\"bonus\"] == max_bonus:\n",
    "        print \"the max bonus person is \", k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOTAL 是总数，这个是典型的异常值，所以一定要拿掉这个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143L, 10L)\n"
     ]
    }
   ],
   "source": [
    "data_dict.pop(\"TOTAL\", 0)\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\", 0)\n",
    "data_dict.pop(\"LOCKHART EUGENE E\", 0)\n",
    "y, X = targetFeatureSplit(featureFormat(data_dict, features_list))\n",
    "X = np.array(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看 bonus或者salary 有没有小于0异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict.items():\n",
    "    if v[\"bonus\"] == 'NaN' or v[\"salary\"] == 'NaN':\n",
    "        continue\n",
    "    if v[\"bonus\"] < 0 or v[\"salary\"] < 0:\n",
    "        print \"the bonus or salary is less than 0 is %s -- %s,%s\"%(k,v[\"bonus\"],v[\"salary\"])\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看来，没有小于0的，再看看有没有 bonus 和 salary 超级多的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 2 max salary abd bonus is LAY KENNETH L -- 1072321,7000000\n",
      "the 2 max salary abd bonus is SKILLING JEFFREY K -- 1111258,5600000\n"
     ]
    }
   ],
   "source": [
    "for k,v in data_dict.items():\n",
    "    if v[\"salary\"] == 'NaN' or v[\"bonus\"] == 'NaN':\n",
    "        continue\n",
    "    if v[\"salary\"] > 1000000 and v[\"bonus\"] > 5000000:\n",
    "        print \"the 2 max salary abd bonus is %s -- %s,%s\"%(k,v[\"salary\"],v[\"bonus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现有2个人，这分别是大名鼎鼎的 Kenneth Lay 和 Jeffrey Skilling，很显然，他们不是异常值，他们是正常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务3: 创建新特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于项目要求对比创建新特征前后，最后结果对比，由于机器学习算法很多，我就用朴素贝叶斯算法来做对比的，因为简单，我先记录下使用老特征用朴素贝叶斯的执行结果 f1。\n",
    "\n",
    "因为 f1 是 percision 和 recall 的综合得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features  (143L, 10L)\n",
      "lables  143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elnin\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use only old feature f1 is:  0.321380952381\n"
     ]
    }
   ],
   "source": [
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "y, X = targetFeatureSplit(data)\n",
    "X = np.array(X)\n",
    "print \"features \", X.shape\n",
    "print \"lables \", len(y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "cv = StratifiedShuffleSplit(y, 100, random_state=42)\n",
    "naive_clf = GaussianNB()\n",
    "parameters = {}\n",
    "naive_clf_grid = GridSearchCV(naive_clf, parameters, cv=cv, scoring='f1')\n",
    "naive_clf_grid.fit(X, y)\n",
    "print \"use only old feature f1 is: \", naive_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始创建新特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "old_features_list = features_list\n",
    "\n",
    "def isnan(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加 从poi邮箱中收取邮件的百分比 和 发送到poi邮箱中的百分比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in my_dataset.items():\n",
    "    if float(v['to_messages']) == 0.: \n",
    "        v['from_poi_to_this_ratio'] = 0.\n",
    "    else: \n",
    "        v['from_poi_to_this_ratio'] = float(v['from_poi_to_this_person']) / float(v['to_messages'])\n",
    "    if float(v['from_messages']) == 0.: \n",
    "        v['from_this_to_poi_ratio'] = 0.\n",
    "    else:\n",
    "        v['from_this_to_poi_ratio'] =  float(v['from_this_person_to_poi']) / float(v['from_messages'])\n",
    "    if isnan(v['from_poi_to_this_ratio']): v['from_poi_to_this_ratio'] = 'NaN'\n",
    "    if isnan(v['from_this_to_poi_ratio']): v['from_this_to_poi_ratio'] = 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加 奖金和工资的倍数，也就是 `bonus / salary`。按照常识，这个一般奖金按照公司系数发放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in my_dataset.items():\n",
    "    #print '=========== ',k\n",
    "    if float(v['salary']) == 0.: \n",
    "        v['bonus_times'] = 0.\n",
    "    else: \n",
    "        v['bonus_times'] = float(v['bonus']) / float(v['salary'])\n",
    "    if isnan(v['bonus_times']): \n",
    "        v['bonus_times'] = 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features  (143L, 13L)\n",
      "lables  143\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "features_list = old_features_list + ['from_this_to_poi_ratio', 'from_poi_to_this_ratio', 'bonus_times']\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "y, X = targetFeatureSplit(data)\n",
    "X = np.array(X)\n",
    "print \"features \", X.shape\n",
    "print \"lables \", len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征增加了，再次用决策树看看，重要相关特征有哪些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.feature_importances_ :  [ 0.04237037  0.          0.02814706  0.04237037  0.          0.          0.\n",
      "  0.27237019  0.272119    0.08308696  0.25953606  0.          0.        ]\n",
      "importance  0  -  expenses  -  0.272370186335\n",
      "importance  1  -  exercised_stock_options  -  0.272118995\n",
      "importance  2  -  from_this_to_poi_ratio  -  0.259536062579\n",
      "importance  3  -  deferred_income  -  0.0830869565217\n",
      "importance  4  -  long_term_incentive  -  0.0423703703704\n",
      "importance  5  -  bonus  -  0.0423703703704\n",
      "importance  6  -  restricted_stock  -  0.0281470588235\n",
      "importance  7  -  bonus_times  -  0.0\n",
      "importance  8  -  from_poi_to_this_ratio  -  0.0\n",
      "importance  9  -  other  -  0.0\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=12)\n",
    "clf = clf.fit(X, y)\n",
    "print \"clf.feature_importances_ : \", clf.feature_importances_\n",
    "idx_feature_importances = np.argsort(clf.feature_importances_)[::-1]\n",
    "for i in range(10):\n",
    "    idx = idx_feature_importances[i]\n",
    "    print \"importance \", i, \" - \", features_list[idx+1], \" - \", clf.feature_importances_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的内容，我把特征的范围再缩小一下，只用决策树有用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'expenses', 'exercised_stock_options', 'from_this_to_poi_ratio', 'deferred_income', 'long_term_incentive', 'bonus', 'restricted_stock']\n",
      "features  (141L, 7L)\n",
      "lables  141\n"
     ]
    }
   ],
   "source": [
    "new_features_list = ['poi']\n",
    "for i in range(7):\n",
    "    idx = idx_feature_importances[i]\n",
    "    new_features_list.append(features_list[idx+1])\n",
    "features_list = new_features_list\n",
    "print new_features_list\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, remove_NaN=True)\n",
    "y, X = targetFeatureSplit(data)\n",
    "X = np.array(X)\n",
    "print \"features \", X.shape\n",
    "print \"lables \", len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务4: 尝试各种各样的分类器模型\n",
    "\n",
    "初始化一些公有的东西,包括PCA转换，MinMaxScalar 还有 StratifiedShuffleSplit\n",
    "\n",
    "**为了节约性能** StratifiedShuffleSplit 我只设置了100次的迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "pca = PCA(svd_solver='randomized', random_state=42)\n",
    "cv = StratifiedShuffleSplit(y, 100, random_state=42)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面开始尝试各种各样的算法， 对于每一种算法，我分别尝试 PAC不转化 和 PAC转化的\n",
    "\n",
    "由于项目要求的 precision 和 recall，我采用f1 score作为GridSearchCV的评估标准，原因是\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall) 这样f1 score同时兼顾了 precision 和 recall 两个指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.413857142857\n",
      "after pca f1:  0.377666666667\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_clf = GaussianNB()\n",
    "parameters = {}\n",
    "naive_clf_grid = GridSearchCV(naive_clf, parameters, cv=cv, scoring='f1')\n",
    "naive_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", naive_clf_grid.best_score_\n",
    "\n",
    "pca_naive_clf = Pipeline([('pca', pca),('svc', naive_clf)])\n",
    "parameters = {'pca__n_components': range(1,5)}\n",
    "pca_naive_clf_grid = GridSearchCV(pca_naive_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_naive_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_naive_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果出来，可以和前面的特征来对比，使用老特征的f1分数是 `0.321380952381` 使用新特征的f1分数是 `0.413857142857` 可以看出，使用新添加的特征后，结果是好于老特征的。\n",
    "\n",
    "### 尝试决策树算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.208412698413\n",
      "after pca f1:  0.38946031746\n"
     ]
    }
   ],
   "source": [
    "# DissionTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=12)\n",
    "parameters = {'max_depth': range(3,10)}\n",
    "tree_clf_grid = GridSearchCV(tree_clf, parameters, cv=cv, scoring='f1')\n",
    "tree_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", tree_clf_grid.best_score_\n",
    "\n",
    "pca_tree_clf = Pipeline([('pca', pca), ('svc', tree_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'svc__max_depth': range(3,10)}\n",
    "pca_tree_clf_grid = GridSearchCV(pca_tree_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_tree_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_tree_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试KNN算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.267\n",
      "after pca f1:  0.314333333333\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors': [3,5,7,10,15],\n",
    "              'weights': ['uniform', 'distance']}\n",
    "knn_clf_grid = GridSearchCV(knn_clf, parameters, cv=cv, scoring='f1')\n",
    "knn_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", knn_clf_grid.best_score_\n",
    "\n",
    "pca_knn_clf = Pipeline([('pca', pca), ('clf', knn_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'clf__n_neighbors': [3,5,7,10,15],\n",
    "              'clf__weights': ['uniform', 'distance']}\n",
    "pca_knn_clf_grid = GridSearchCV(pca_knn_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_knn_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_knn_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试一些集成学习算法，AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.434714285714\n",
      "after pca f1:  0.361714285714\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
    "parameters = {'n_estimators': [25,50,75,100],\n",
    "              'learning_rate': [0.01, 0.1, 1]}\n",
    "adaboost_clf_grid = GridSearchCV(adaboost_clf, parameters, cv=cv, scoring='f1')\n",
    "adaboost_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", adaboost_clf_grid.best_score_\n",
    "\n",
    "pca_adaboost_clf = Pipeline([('pca', pca), ('svc', adaboost_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'svc__n_estimators': [25,50,75,100],\n",
    "              'svc__learning_rate': [0.01, 0.1, 1]}\n",
    "pca_adaboost_clf_grid = GridSearchCV(pca_adaboost_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_adaboost_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_adaboost_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试一些集成学习算法，RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pca f1:  0.386095238095\n",
      "after pca f1:  0.343380952381\n"
     ]
    }
   ],
   "source": [
    "# RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = AdaBoostClassifier(random_state=42)\n",
    "parameters = {'n_estimators': [5,10,15,20]}\n",
    "forest_clf_grid = GridSearchCV(forest_clf, parameters, cv=cv, scoring='f1')\n",
    "forest_clf_grid.fit(X, y)\n",
    "print \"before pca f1: \", forest_clf_grid.best_score_\n",
    "\n",
    "pca_forest_clf = Pipeline([('pca', pca), ('svc', forest_clf)])\n",
    "parameters = {'pca__n_components': range(1,5),\n",
    "              'svc__n_estimators': [5,10,15,20]}\n",
    "pca_forest_clf_grid = GridSearchCV(pca_forest_clf, parameters, cv=cv, scoring='f1')\n",
    "pca_forest_clf_grid.fit(X, y)\n",
    "print \"after pca f1: \", pca_forest_clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务5：用分类器做预测\n",
    "\n",
    "根据之上尝试过的算法来看，大致看来，使用朴素贝叶斯且不经过PCA转换的f1 score效果最好。\n",
    "\n",
    "所以我将最终算法设定为朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final clf:  GaussianNB(priors=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "clf = naive_clf_grid.best_estimator_\n",
    "print \"final clf: \", clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面拆分训练集和测试集数据 来简单验证一下，看看相关分数是多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.883720930233  precision: 0.5  recall: 0.6  f1: 0.545454545455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print \"accuracy: \",accuracy, \" precision:\",precision, \" recall:\", recall, \" f1:\", f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见其，测试下载，其 precision 和 recall 分别能做到 0.5 和 0.6\n",
    "\n",
    "另外，运行 tester.py 其结果如下：\n",
    "\n",
    "```\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.85767\tPrecision: 0.45841\tRecall: 0.37200\tF1: 0.41071\tF2: 0.38657\n",
    "\tTotal predictions: 15000\tTrue positives:  744\tFalse positives:  879\tFalse negatives: 1256\tTrue negatives: 12121\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务6: 把分类器，数据集，和特征列表存在本地，分别是\n",
    "\n",
    "- my_classifier.pkl 是最终选择的分类器\n",
    "- my_dataset.pkl 是数据集\n",
    "- my_feature_list.pkl 是选择的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
